Ok, was will ich hier ausprobieren? 
Ich habe eine Menge Tagebucheinträge - Dokumente, in denen ich über meine eigene Situation schreibe. 
Ich fände es cool, einen Zeitstrahl zu sehen, in dem sichbar ist, 
- wie viel ich in dem Monat jeweils geschrieben habe
- wie die Mood war
	=> Für die Mood verwende ich irgend ein AI mood framework. 
- welche Themen in welchem Monat relevant waren (auch dafür kann ich mal schauen, was es für frameworks gibt) 

Muss mal schauen, was es für mood frameworks gibt und in welcher sprache sie geschrieben sind. Potenziell Pyhon. Vielleich aber auch go - irgendwas, was tensorflow hat. 

Analyse
- Für jedes Dokument
	- Lese das Datum vom Dateinamen ab.
	- Bestimme die Mood der Datei
	- Speichere sie als wert Mood(t) in einer Datenbank
		- measure, individuum, value, timestamp

Backend
- Veröffentliche die Daten in einer API
	- get(measure, start, end). E.g. get("mood", 2017-01-01, 2017-12-31)

Frontend
- Schreibe einen kleinen Client, der die Daten anfragt und anzeigt als schönes SVG-Diagramm zeichnet
	- Und zwar: mittelwert UND varianz. Es ist doch interessant zu sehen, wenn die Mood in einem Monat abwechselnd sehr gut und sehr schlecht war. 

Entweder das Frontend oder das Backend muss bitte interpolieren. 


Bevor ich krassen AI-Shit mache, könnte ich erstmal ein cooles interface schreiben, in dem man den verlauf der Themen sieht. 
Vielleicht kann man dann Gruppen von Wörtern bilden, die man verfolgen will. 
Zum Beispiel
 - Unsicherheit: vielleicht, "weiß nicht"
 

Dann könnte ich Wörter wie folgt zählen: 
 - erstmal zähle ich häufige wörter und filtere die 10 häufigsten raus. 
 - dann kombiniere ich jedes Wort, das zu den 5% häufigsten gehört mit den 10 vorherigen wörtern (nur die, die auch zu den 5% häufigsten gehören)  (Kombinationskernel)

Und man kann dann "add to group" -> "uncertainty" klicken und damit sein mood dashboard anpassen. 


EMAILS! Yea! Man kann seine versendeten emails analysieren. die emails, die man versendet nach Themen und mood analysieren über den Verlauf der Zeit. 



Idee: Einen Batch von texten mit Datum normalisiere ich gegen den gesamten batch
Und dann wäre es geil ein diagramm von links nach rechts zu haben, das ich scrollen kann. Und bei dem Diagramm sind immer wieder Hügel für die wichtigsten Themen in dem Bereich. Und an jedem Maximum aber maximal einmal auf der Seite ist ein Label für den Hügel. 

Awesome. 


Also: wie interagiert der User mit der ganzen Geschichte? Er schreibt hin und wieder in sein tägliches Dokument. (Ich könnte halt auch sagen, dass der User posts macht. D.h. auf enter drückt, nachdem er fertig geschrieben hat. Das wäre einfacher zu realisieren, aber nicht so bequem - man könnte Änderungen verlieren, wenn man vergisst abzuschicken oder es technische Probleme gibt. Und man kann nicht editieren - das mache ich aber eh kaum. Also let's go, machen wir es post-basiert - als progressive web app?) 
Jeder Post bekommt seinen timestamp. 
Wie berechne ich in Abhängigkeit davon jetzt die topic density? 
A) Ich berechne die word frequencies für jeden Post und interpoliere dann
B) Ich fahre einen Kernel mit einer bestimmten Breite über die Zeitachse und addiere die word counts und total words (zum rand weniger gewichtet) und normiere für jede position. Yea, nice.

Ich nehme B, das ist geil.

D.h. ich brauche: 
- Die reference (i.d.R. die gesamtmenge an text) => ratio pro Wort (d.h. count pro wort und total count => kann sukzessive in einer db gepflegt werden) 
- Dann Parameter: 
	- Von wann bis wann? 
	- Wort (non-mvp: oder Gruppe von Wörtern, die von interesse sind (wenn Gruppe von Wörtern => eg. "uncertainty"-Group count wird inkrementiert, immer wenn eines der Wörter "vielleicht", "womöglich", "unsicher", ... gelesen wird))
- die Posts von (startDate - kernelRadius) bis (endDate + kernelRadius) 

Was ist mein Ziel im ersten Schritt?
Eine JS Funktion schreiben, die ein svg generiert entsprechend der parameter. Ohne website. Soll einfach direkt so laufen. Node. Soll die Textdateien einfach einlesen. Dafür schreibe ich eine Komponente, die eine funktion "next" bietet, die einfach den nächsten post liefert. d.h. timestamp, inhalt & wordcounts. 
Dann schreibe ich eine Komponente, die die wordcounts aus einer textdatei liest. 
(Die zwei Komponenten werde ich später durch eine db ersetzen. Es gibt wirklich ein "import emails")  

Das Hauptprorgamm nutzt die beiden Komponenten. Setzt den cursor auf (startDate - kernelRadius) und läd next posts, bis eines ein späteres Datum als (startDate + kernelRadius) hat.

Ich kann die texte aber auch erstmal in eine mongodb importieren und dann direkt auf der mongodb arbeiten. 

#BIS HIER HER IMPLEMENTIEREN#

In jedem fall: Post Funktion 
	- muss die words counten und mitablegen + total words
	- muss die word counts zur reference addieren + total words auch addieren
	- einträge: user und book. Reference dann pro book anlegen.

Was wäre eine schöne Architektur? 
	- create post
	- get post
	- analyse
		- wo soll die analyse mit dem kernel passieren?
			- Im backend? 
				Vorteile: wenn der Zeitraum groß ist, werden in relation zur Informationsbasis verhältnismäßig wenige Daten übermittelt. 
				Nachteile: wenn der Zeitraum klein ist, werden in Relation verhältnismäßig viele Daten übermittelt. 
				Bei jedem Navigieren des Clients wird eine Anfrage an den Server gestellt
			- Im forntend?
				Beim Navigieren muss nur gegebenenfalls daten nachgeladen werden (evtl. kompliziert)
		- als progressive web app wäre das natürlich cool - aber das ist noch nicht ganz ausgereift. Es wäre schon geil... 
			und es würde die Frage von oben beantworten. Analyse würde immer auf der Clientseite durchgeführt werden.
			OK, progressive web app. here we go.
			- Posting und Analyse auf der Clientseite 
			- Posting sync über server 

Wenn der Client etwas analysieren möchte: 
	- der service worker fragt einen Timeframe ab
	- der server returned eine liste der betroffenen IDs
		- wenn der service worker einen post einer bestimmten ID noch nicht hat, dann fragt er diese speziellen Posts ab

Hmm, diese Dreiteilung möchte ich jetzt mal untersuchen und spezifizieren: 
- Client: 
	- Erlaubt es einen Post zu formulieren
	- Erlaubt es analysedaten zu rendern
- Service Worker
	- post(text) 
		- persistiert die posts lokal
		- versuch, an den server zu senden
	- getAnalysisData(from, to, kernelSize, stepSize)
		- checken, 
			- dass (to - from) / stepSize vernünftig ist
			- dass (to - from) / kernelSize vernünftig ist
		- posts aus dem Zeitfenster abfragen aus ner mongodb 
		- kernel darüber bewegen
			- Liste führen für alle posts die gerade im Kernel liegen
	- pre-analysiert die posts lokal: word counts + total words
	- sobald dem Server alle lokalen changes mitgeteilt wurden, fragt er vom Server die neuen global stats für das book ab und überschreibt damit den lokalen Stand.
	
Clients pullen updates, wenn sie gestartet werden oder wenn analyse Anfragen an den Service Worker gestellt werden. 

#analysis
- Welche keywords sind interessant, also was inkludiere ich in den analyseergebnissen? alles, was über 2 und unter 0.5 ist? ja, das klingt gut.
	- Dafür muss ich erstmal eine Liste betreiben: wenn in irgendeinem Step ein Wort den Threshold überschreitet, nehme ich es in die Liste auf.
	- Dann, beim Ergebnisse versenden, verkürze ich jede Liste pro Step auf die relevanten Wörter
	- Im Ergebnis wird auch eine Liste aller relevanten wörtern mit 
		- maxvalue & respectiveStep verschickt
		- minvalue & respectiveStep verschickt
		- Diese Infos kann der Client fürs labelling verwenden + als info anzeigen (e.g. 5 most outstanding topics vs. 5 least outstanding topics => click brings you to the graphic) 
- Beim einfügen muss noch das book erstellt werden + die Wordcounts gemacht werden + die book stats geupdated werden (immer per word & gesamt) 

#bugs
- replace non-breaking spaces. There is also lots of commas and shit still going on in the counted words.

#resultrendering
- 3 Möglichkeiten: 
	- Immer ganzes svg rendern
	- Svg editieren
	- Svg Pages shiften
	- Ich denke ich rendere das ganze svg und vertraue auf die Fähigkeiten des Browsers geclippte Inhalte nicht zu rendern... denn ob ich es jetzt in pages unterteile, oder der browser einen spacial index anwendet um die zu rendernden bereiche auszuwählen macht keinen großen Unterschied bezügl. der Performance. D.h. der universelle Ansatz ist hier annährend so gut wie ein spezifischer Ansatz und damit falle ich gern auf den universellen Ansatz zurück. 


### IDEEN danach: ###
- word counts sind geil. Ein text durch wordcounts dargestellt ist meistens kleiner als der text selbst. 
- bei den wordcounts kann ich kreuzprodukte untersuchen
- testen und showcasen kann ich das tool indem ich Bücher analysiere - jede Seite ist 1 Stunde oder so. 

### NEXT STEPS ###
- Make import script use the server
- zum besseren ausprobieren: 
	- weniger schritte
- book normalisierung
	- beim posten ins book eintragen
- gewichtung funktioniert noch nicht
